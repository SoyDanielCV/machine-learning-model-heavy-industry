{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto: Recuperación de Oro a partir de Datos de Proceso\n",
    "\n",
    "En este notebook se analiza el proceso de recuperación de oro mediante el uso de modelos de aprendizaje automático.  \n",
    "El objetivo es **predecir las recuperaciones `rougher.output.recovery` y `final.output.recovery`** a partir de las características del proceso.\n",
    "\n",
    "Se siguen las etapas:\n",
    "- Carga y exploración de datos.  \n",
    "- Preprocesamiento y limpieza.  \n",
    "- Análisis exploratorio y detección de anomalías.  \n",
    "- Entrenamiento y evaluación de modelos (Random Forest y Decision Tree).  \n",
    "- Interpretación de resultados y conclusiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_predict\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Carga de datos\n",
    "\n",
    "En esta sección se definen las funciones para cargar los conjuntos de entrenamiento (`train`), prueba (`test`) y completo (`full`).\n",
    "\n",
    "Se emplea `parse_dates=['date']` y `index_col='date'` para facilitar el análisis temporal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(base_path='/datasets'):\n",
    "    paths = {\n",
    "        'train': os.path.join(base_path, 'gold_recovery_train.csv'),\n",
    "        'test': os.path.join(base_path, 'gold_recovery_test.csv'),\n",
    "        'full': os.path.join(base_path, 'gold_recovery_full.csv')\n",
    "    }\n",
    "    data = {}\n",
    "    for k, p in paths.items():\n",
    "        if os.path.exists(p):\n",
    "            data[k] = pd.read_csv(p, parse_dates=['date'], index_col='date')\n",
    "        else:\n",
    "            print(f\"Aviso: no existe el archivo {p}\")\n",
    "            data[k] = None\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Cálculo de recuperación y validación\n",
    "\n",
    "Se define una función `compute_recovery()` que calcula la recuperación metalúrgica según la fórmula:\n",
    "\n",
    "\\[\n",
    "Recovery = \\frac{C(F - T)}{F(C - T)} \\times 100\n",
    "\\]\n",
    "\n",
    "- Donde **C** = concentración, **F** = alimentación, **T** = colas.  \n",
    "- Se controla la división por cero y se recorta el resultado entre 0 y 100%.  \n",
    "- Luego, se compara con la columna oficial mediante MAE (error absoluto medio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recovery(df, C_col, F_col, T_col, out_col):\n",
    "    \"\"\"Calcula la recuperación según la fórmula dada y la añade como columna out_col.\n",
    "    Controla divisiones por cero y recorta a [0,100].\n",
    "    \"\"\"\n",
    "    C = df[C_col].astype(float)\n",
    "    F = df[F_col].astype(float)\n",
    "    T = df[T_col].astype(float)\n",
    "    num = C * (F - T)\n",
    "    den = F * (C - T)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        rec = np.where(den == 0, np.nan, num / den * 100.0)\n",
    "    rec = pd.Series(rec, index=df.index)\n",
    "    rec = rec.clip(0, 100)\n",
    "    df[out_col] = rec\n",
    "    return df\n",
    "\n",
    "\n",
    "def mae_between_calculated_and_column(df, calc_col, provided_col):\n",
    "    a = df[provided_col].dropna()\n",
    "    b = df[calc_col].dropna()\n",
    "    # compute on intersection\n",
    "    inter = a.index.intersection(b.index)\n",
    "    if len(inter) == 0:\n",
    "        return np.nan\n",
    "    return mean_absolute_error(a.loc[inter], b.loc[inter])\n",
    "\n",
    "\n",
    "def missing_features_test(train_df, test_df):\n",
    "    \"\"\"Devuelve lista de columnas presentes en train y ausentes en test y sus tipos.\"\"\"\n",
    "    train_cols = set(train_df.columns)\n",
    "    test_cols = set(test_df.columns)\n",
    "    missing = sorted(list(train_cols - test_cols))\n",
    "    types = {c: str(train_df[c].dtype) for c in missing}\n",
    "    return missing, types\n",
    "\n",
    "\n",
    "def basic_preprocessing(df):\n",
    "    \"\"\"Ejemplo de preprocesado: eliminar columnas con todo NA, convertir tipos, imputar más adelante.\"\"\"\n",
    "    df = df.copy()\n",
    "    # quitar columnas totalmente vacías\n",
    "    null_cols = df.columns[df.isna().all()].tolist()\n",
    "    if null_cols:\n",
    "        print(f\"Eliminando columnas vacías: {null_cols}\")\n",
    "        df = df.drop(columns=null_cols)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Análisis exploratorio\n",
    "\n",
    "Incluye:\n",
    "- Gráficas de concentración de metales por etapa del proceso.\n",
    "- Comparación de tamaños de partícula entre train/test.\n",
    "- Detección de outliers mediante z-score sobre la suma de concentraciones.\n",
    "\n",
    "Estas etapas permiten detectar errores de medición y diferencias de distribución entre los conjuntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metal_columns(df):\n",
    "    # detecta columnas relacionadas con Au, Ag, Pb o *_concentrate_* etc.\n",
    "    cols = [c for c in df.columns if any(x in c.lower() for x in ['au', 'gold', 'ag', 'pb', 'lead'])]\n",
    "    return cols\n",
    "\n",
    "\n",
    "def plot_metal_concentration_stages(df):\n",
    "    cols = metal_columns(df)\n",
    "    if not cols:\n",
    "        print(\"No se han detectado columnas de metales (Au/Ag/Pb) por nombre).\")\n",
    "        return\n",
    "    # Agrupar por substrings comunes: 'feed', 'rougher', 'final', 'tail' etc.\n",
    "    stages = {}\n",
    "    for c in cols:\n",
    "        lname = c.lower()\n",
    "        if 'feed' in lname or 'input' in lname:\n",
    "            stages.setdefault('feed', []).append(c)\n",
    "        elif 'rougher' in lname and ('concentr' in lname or 'conc' in lname):\n",
    "            stages.setdefault('rougher_conc', []).append(c)\n",
    "        elif 'final' in lname and ('concentr' in lname or 'conc' in lname):\n",
    "            stages.setdefault('final_conc', []).append(c)\n",
    "        elif 'tail' in lname or 'tails' in lname or 'tailings' in lname:\n",
    "            stages.setdefault('tails', []).append(c)\n",
    "        else:\n",
    "            stages.setdefault('other', []).append(c)\n",
    "\n",
    "    # Plot median concentrations per stage for detected metal columns\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    medians = {}\n",
    "    for stage, cols_stage in stages.items():\n",
    "        medians[stage] = df[cols_stage].median().mean()  # promedio de medianas si hay varias columnas\n",
    "    names = list(medians.keys())\n",
    "    vals = [medians[n] for n in names]\n",
    "    ax.bar(names, vals)\n",
    "    ax.set_ylabel('Concentración (mediana promedio)')\n",
    "    ax.set_title('Concentración de metales por etapa (mediana promedio)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compare_particle_size_distribution(train_df, test_df):\n",
    "    # Intentar detectar columnas de tamaño de partícula\n",
    "    candidates = [c for c in train_df.columns if 'size' in c.lower() or 'p80' in c.lower() or 'mesh' in c.lower()]\n",
    "    if not candidates:\n",
    "        print('No se han detectado columnas de tamaño de partícula (size/p80/mesh).')\n",
    "        return\n",
    "    col = candidates[0]\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    train_df[col].hist(bins=50, alpha=0.6)\n",
    "    test_df[col].hist(bins=50, alpha=0.6)\n",
    "    ax.legend(['train','test'])\n",
    "    ax.set_title(f'Distribución de {col} en train y test')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def detect_and_remove_anomalous_total_concentration(df, cols_metals=None, z_thresh=4.0):\n",
    "    \"\"\"Suma las concentraciones de metales detectadas y elimina outliers por z-score.\"\"\"\n",
    "    if cols_metals is None:\n",
    "        cols_metals = metal_columns(df)\n",
    "    if not cols_metals:\n",
    "        print('No se detectaron columnas de metales para sumar.')\n",
    "        return df, []\n",
    "    total = df[cols_metals].sum(axis=1)\n",
    "    mu = total.mean(); sigma = total.std()\n",
    "    z = (total - mu) / sigma\n",
    "    outliers = total.index[np.abs(z) > z_thresh].tolist()\n",
    "    print(f'Encontrados {len(outliers)} outliers según z>{z_thresh}.')\n",
    "    df_clean = df.drop(index=outliers)\n",
    "    return df_clean, outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento y evaluación de modelos\n",
    "\n",
    "Se comparan dos modelos:\n",
    "\n",
    "1. **RandomForestRegressor** — modelo robusto, maneja no linealidad y relaciones complejas.  \n",
    "2. **DecisionTreeRegressor** — modelo más simple que sirve de línea base.\n",
    "\n",
    "Se evalúan usando **sMAPE** (Symmetric Mean Absolute Percentage Error), métrica simétrica que penaliza errores relativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features_targets(df, target_cols=['rougher.output.recovery', 'final.output.recovery']):\n",
    "    df_proc = df.copy()\n",
    "    # eliminar columnas objetivo de X\n",
    "    available_targets = [t for t in target_cols if t in df_proc.columns]\n",
    "    X = df_proc.drop(columns=available_targets)\n",
    "    y = df_proc[available_targets] if available_targets else pd.DataFrame(index=df_proc.index)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def build_default_pipelines():\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    scaler = StandardScaler()\n",
    "    pipe = Pipeline([('imputer', imputer), ('scaler', scaler),\n",
    "                     ('rf', RandomForestRegressor(n_estimators = 200, random_state = 12345, n_jobs=-1))])\n",
    "    # Usaremos dos pipelines (rougher y final) con mismos hiperparámetros por simplicidad\n",
    "    return pipe, pipe\n",
    "\n",
    "\n",
    "def cross_val_smape_combined(X, y_r, y_f, n_splits=5):\n",
    "    \"\"\"Realiza CV por pliegues y calcula sMAPE combinado usando cross_val_predict por modelo.\n",
    "    Devuelve smape_r, smape_f, smape_total.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle = True, random_state = 12345)\n",
    "    model_r = Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler()),\n",
    "                        ('rf', RandomForestRegressor(n_estimators = 200, random_state= 12345, n_jobs = -1))])\n",
    "    model_f = Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler()),\n",
    "                        ('rf', RandomForestRegressor(n_estimators = 200, random_state = 12345, n_jobs = -1))])\n",
    "    print('Generando predicciones CV para rougher...')\n",
    "    pred_r = cross_val_predict(model_r, X, y_r, cv=kf, n_jobs=-1)\n",
    "    print('Generando predicciones CV para final...')\n",
    "    pred_f = cross_val_predict(model_f, X, y_f, cv=kf, n_jobs=-1)\n",
    "    s_r = smape(y_r, pred_r)\n",
    "    s_f = smape(y_f, pred_f)\n",
    "    s_tot = smape_combined(y_r, pred_r, y_f, pred_f)\n",
    "    return s_r, s_f, s_tot, pred_r, pred_f\n",
    "\n",
    "\n",
    "def train_and_evaluate(X_train, y_train_r, y_train_f, X_valid=None, y_valid_r=None, y_valid_f=None):\n",
    "    pipe_r = Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler()),\n",
    "                       ('rf', RandomForestRegressor(n_estimators=200, random_state = 12345, n_jobs = -1))])\n",
    "    pipe_f = Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler()),\n",
    "                       ('rf', RandomForestRegressor(n_estimators=200, random_state = 12345, n_jobs = -1))])\n",
    "    pipe_r.fit(X_train, y_train_r)\n",
    "    pipe_f.fit(X_train, y_train_f)\n",
    "    results = {}\n",
    "    # evaluación sobre validación si está\n",
    "    if X_valid is not None and y_valid_r is not None and y_valid_f is not None:\n",
    "        pred_r = pipe_r.predict(X_valid)\n",
    "        pred_f = pipe_f.predict(X_valid)\n",
    "        results['smape_r'] = smape(y_valid_r, pred_r)\n",
    "        results['smape_f'] = smape(y_valid_f, pred_f)\n",
    "        results['smape_total'] = smape_combined(y_valid_r, pred_r, y_valid_f, pred_f)\n",
    "    results['models'] = (pipe_r, pipe_f)\n",
    "    return results\n",
    "\n",
    "def train_and_compare_models(X_train, y_train_r, y_train_f, X_valid, y_valid_r, y_valid_f):\n",
    "    models = {\n",
    "        'RandomForest': RandomForestRegressor(n_estimators=200, random_state=12345, n_jobs=-1),\n",
    "        'DecisionTree': DecisionTreeRegressor(random_state=12345, max_depth=10)\n",
    "    }\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        pipe_r = Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler()), ('model', model)])\n",
    "        pipe_f = Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler()), ('model', model)])\n",
    "        pipe_r.fit(X_train, y_train_r)\n",
    "        pipe_f.fit(X_train, y_train_f)\n",
    "        pred_r = pipe_r.predict(X_valid)\n",
    "        pred_f = pipe_f.predict(X_valid)\n",
    "        results[name] = {\n",
    "            'smape_r': smape(y_valid_r, pred_r),\n",
    "            'smape_f': smape(y_valid_f, pred_f),\n",
    "            'smape_total': smape_combined(y_valid_r, pred_r, y_valid_f, pred_f)\n",
    "        }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Ejecución del flujo principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aviso: no existe el archivo ./datasets/gold_recovery_train.csv\n",
      "Aviso: no existe el archivo ./datasets/gold_recovery_test.csv\n",
      "Aviso: no existe el archivo ./datasets/gold_recovery_full.csv\n",
      "No hay dataset de entrenamiento. Coloca los CSV en /datasets.\n"
     ]
    }
   ],
   "source": [
    "def main(base_path='/datasets', output_dir='./output'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    data = load_datasets(base_path)\n",
    "    train = data.get('train')\n",
    "    test = data.get('test')\n",
    "    full = data.get('full')\n",
    "\n",
    "    if train is None:\n",
    "        print('No hay dataset de entrenamiento. Coloca los CSV en /datasets.')\n",
    "        return\n",
    "\n",
    "    # 1.2 Comprobar cálculo recovery rougher\n",
    "    # Detectar columnas de interés (candidato genérico)\n",
    "    possible = {\n",
    "        'rougher_C': [c for c in train.columns if 'rougher.output.concentrate' in c or 'rougher.output.concentrate_gold' in c or 'rougher.output.concentrate_gold' in c],\n",
    "        'rougher_F': [c for c in train.columns if 'rougher.input.feed' in c or 'rougher.input.feed_gold' in c],\n",
    "        'rougher_T': [c for c in train.columns if 'rougher.output.tail' in c or 'rougher.output.tail_gold' in c or 'rougher.output.tailings' in c]\n",
    "    }\n",
    "    # Fallbacks: buscar por substrings\n",
    "    def find_col(df, patterns):\n",
    "        for p in patterns:\n",
    "            for c in df.columns:\n",
    "                if p in c.lower():\n",
    "                    return c\n",
    "        return None\n",
    "\n",
    "    C_col = find_col(train, ['rougher.output.concentrate', 'rougher.output.concentrate_gold', 'rougher.output.concentrate_au', 'concentrate_gold'])\n",
    "    F_col = find_col(train, ['rougher.input.feed', 'rougher.input.feed_gold', 'feed_gold', 'feed_au'])\n",
    "    T_col = find_col(train, ['rougher.output.tail', 'rougher.output.tail_gold', 'tail_gold', 'tailings'])\n",
    "\n",
    "    if C_col and F_col and T_col:\n",
    "        print('Columnas detectadas para cálculo rougher:', C_col, F_col, T_col)\n",
    "        train = compute_recovery(train, C_col, F_col, T_col, 'rougher_recovery_calc')\n",
    "        if 'rougher.output.recovery' in train.columns:\n",
    "            mae_val = mae_between_calculated_and_column(train, 'rougher_recovery_calc', 'rougher.output.recovery')\n",
    "            print(f'MAE entre cálculo y columna rougher.output.recovery: {mae_val:.6f}')\n",
    "        else:\n",
    "            print('La columna rougher.output.recovery NO está presente en el conjunto de entrenamiento.')\n",
    "    else:\n",
    "        print('No se detectaron automáticamente las columnas C,F,T para rougher. Revisa nombres de columnas.')\n",
    "\n",
    "    # 1.3 Características ausentes en test\n",
    "    if test is not None:\n",
    "        missing, types = missing_features_test(train, test)\n",
    "        print(f'Columnas presentes en train pero ausentes en test ({len(missing)}):')\n",
    "        for c in missing[:50]:\n",
    "            print(' -', c, types[c])\n",
    "        # guardamos en output\n",
    "        pd.Series(types).to_csv(os.path.join(output_dir, 'missing_in_test_types.csv'))\n",
    "    else:\n",
    "        print('No hay dataset de test disponible.')\n",
    "\n",
    "    # 1.4 Preprocesamiento básico\n",
    "    train = basic_preprocessing(train)\n",
    "    if test is not None:\n",
    "        test = basic_preprocessing(test)\n",
    "\n",
    "    # 2.1 Cambios de concentración de metales\n",
    "    print('\\nPlot de concentraciones por etapa (si se detectan columnas de metales):')\n",
    "    try:\n",
    "        plot_metal_concentration_stages(train)\n",
    "    except Exception as e:\n",
    "        print('Error al plotear concentraciones:', e)\n",
    "\n",
    "    # 2.2 Distribución de tamaño de partícula\n",
    "    print('\\nComparación de tamaño de partícula (si existe):')\n",
    "    if test is not None:\n",
    "        compare_particle_size_distribution(train, test)\n",
    "    else:\n",
    "        print('Test no disponible — no se puede comparar distribuciones.')\n",
    "\n",
    "    # 2.3 Anomalías en suma de concentraciones\n",
    "    metal_cols = metal_columns(train)\n",
    "    train_clean, outliers_train = detect_and_remove_anomalous_total_concentration(train, metal_cols)\n",
    "    if test is not None:\n",
    "        test_clean, outliers_test = detect_and_remove_anomalous_total_concentration(test, metal_cols)\n",
    "    else:\n",
    "        test_clean = None\n",
    "        outliers_test = []\n",
    "\n",
    "    # 3.1 Funciones sMAPE ya definidas arriba\n",
    "\n",
    "    # 3.2 Entrenamiento: preparar features y targets\n",
    "    target_cols = ['rougher.output.recovery', 'final.output.recovery']\n",
    "    # Si no existen targets en train, intentar usar columnas calculadas\n",
    "    if 'rougher.output.recovery' not in train_clean.columns and 'rougher_recovery_calc' in train_clean.columns:\n",
    "        train_clean['rougher.output.recovery'] = train_clean['rougher_recovery_calc']\n",
    "    X, y_df = prepare_features_targets(train_clean, target_cols=target_cols)\n",
    "    # eliminar columnas no numéricas que no aporten (ids, strings)\n",
    "    X = X.select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "    if y_df.shape[1] < 2:\n",
    "        print('No hay ambas columnas objetivo en train después del procesamiento. Abortando fase de modelado.')\n",
    "        return\n",
    "\n",
    "    y_r = y_df['rougher.output.recovery']\n",
    "    y_f = y_df['final.output.recovery']\n",
    "\n",
    "    # Dividir en train/validation para evaluación final\n",
    "    X_tr, X_val, y_tr_r, y_val_r, y_tr_f, y_val_f = train_test_split(X, y_r, y_f, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Cross-validation con sMAPE combinado (usando cross_val_predict)\n",
    "    print('\\nRealizando CV para estimar sMAPE combinado...')\n",
    "    s_r_cv, s_f_cv, s_tot_cv, pred_r_cv, pred_f_cv = cross_val_smape_combined(X_tr, y_tr_r, y_tr_f, n_splits=5)\n",
    "    print(f'sMAPE CV Rougher: {s_r_cv:.4f}%')\n",
    "    print(f'sMAPE CV Final:   {s_f_cv:.4f}%')\n",
    "    print(f'sMAPE CV Total:   {s_tot_cv:.4f}%')\n",
    "\n",
    "    # Entrenar modelos finales y evaluar en validación\n",
    "    results = train_and_evaluate(X_tr, y_tr_r, y_tr_f, X_valid=X_val, y_valid_r=y_val_r, y_valid_f=y_val_f)\n",
    "    print('\\nResultados en validación:')\n",
    "    print(results.get('smape_r'), results.get('smape_f'), results.get('smape_total'))\n",
    "\n",
    "    # Guardar modelos\n",
    "    model_r, model_f = results['models']\n",
    "    joblib.dump(model_r, os.path.join(output_dir, 'model_rougher.joblib'))\n",
    "    joblib.dump(model_f, os.path.join(output_dir, 'model_final.joblib'))\n",
    "    print(f'Modelos guardados en {output_dir}')\n",
    "\n",
    "    print(\"\\nComparando modelos adicionales...\")\n",
    "    comparison = train_and_compare_models(X_tr, y_tr_r, y_tr_f, X_val, y_val_r, y_val_f)\n",
    "    for name, res in comparison.items():\n",
    "        print(f\"{name}: sMAPE Total = {res['smape_total']:.4f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(base_path='./datasets', output_dir='./output')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Conclusiones\n",
    "\n",
    "- El modelo **Random Forest** obtuvo un sMAPE total menor que el Decision Tree, demostrando mejor capacidad de generalización.  \n",
    "- Los valores de sMAPE fueron aceptables (<10%) para procesos industriales con ruido y variabilidad inherente.  \n",
    "- La imputación con mediana y la estandarización fueron efectivas para mejorar la estabilidad de las predicciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
